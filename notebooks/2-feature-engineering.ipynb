{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SmartPave Analytics: Feature Engineering\n",
        "\n",
        "## Overview\n",
        "This notebook creates advanced features for machine learning models to predict pavement degradation and optimize maintenance costs.\n",
        "\n",
        "## Objectives\n",
        "- Create time-based features\n",
        "- Engineer traffic impact features\n",
        "- Develop weather impact metrics\n",
        "- Create maintenance history features\n",
        "- Prepare data for ML modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data from Snowflake and create features\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Connect to Snowflake\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "session = get_active_session()\n",
        "\n",
        "# Set database and schema context\n",
        "session.sql(\"USE DATABASE DOT_workshop_test\").collect()\n",
        "session.sql(\"USE SCHEMA smartpave_analytics\").collect()\n",
        "\n",
        "# Load pavement condition data\n",
        "df = session.sql(\"SELECT * FROM DOT_workshop_test.smartpave_analytics.pavement_condition\").to_pandas()\n",
        "\n",
        "# Detect the actual date column name (Snowflake uses uppercase)\n",
        "date_col = None\n",
        "for col in df.columns:\n",
        "    if col.upper() == 'DATE':\n",
        "        date_col = col\n",
        "        break\n",
        "\n",
        "if date_col:\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    print(f\"Loaded dataset: {len(df):,} records\")\n",
        "    print(f\"Date range: {df[date_col].min()} to {df[date_col].max()}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No date column found\")\n",
        "\n",
        "# Detect segment ID column\n",
        "segment_id_col = None\n",
        "for col in df.columns:\n",
        "    if col.upper() == 'SEGMENT_ID':\n",
        "        segment_id_col = col\n",
        "        break\n",
        "\n",
        "if segment_id_col:\n",
        "    print(f\"Unique segments: {df[segment_id_col].nunique():,}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No segment ID column found\")\n",
        "\n",
        "# Load maintenance data for feature engineering\n",
        "maintenance_df = session.sql(\"SELECT * FROM DOT_workshop_test.smartpave_analytics.maintenance_records\").to_pandas()\n",
        "if 'DATE' in maintenance_df.columns:\n",
        "    maintenance_df['DATE'] = pd.to_datetime(maintenance_df['DATE'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set database and schema context\n",
        "session.sql(\"USE DATABASE DOT_workshop_test\").collect()\n",
        "session.sql(\"USE SCHEMA smartpave_analytics\").collect()\n",
        "\n",
        "# Load data from Snowflake tables\n",
        "print(\"Loading data from Snowflake...\")\n",
        "\n",
        "# Load pavement condition data\n",
        "condition_df = session.sql(\"SELECT * FROM DOT_workshop_test.smartpave_analytics.pavement_condition\").to_pandas()\n",
        "print(f\"Condition data: {len(condition_df):,} records\")\n",
        "\n",
        "# Load road network data\n",
        "roads_df = session.sql(\"SELECT * FROM DOT_workshop_test.smartpave_analytics.road_network\").to_pandas()\n",
        "print(f\"Road network data: {len(roads_df):,} records\")\n",
        "\n",
        "# Load maintenance records\n",
        "maintenance_df = session.sql(\"SELECT * FROM DOT_workshop_test.smartpave_analytics.maintenance_records\").to_pandas()\n",
        "print(f\"Maintenance data: {len(maintenance_df):,} records\")\n",
        "\n",
        "# Load traffic data\n",
        "traffic_df = session.sql(\"SELECT * FROM DOT_workshop_test.smartpave_analytics.traffic_data\").to_pandas()\n",
        "print(f\"Traffic data: {len(traffic_df):,} records\")\n",
        "\n",
        "print(\"\\nData loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data preprocessing and column detection\n",
        "print(\"Preprocessing data and detecting columns...\")\n",
        "\n",
        "# Detect column names (Snowflake stores in uppercase)\n",
        "date_col = None\n",
        "condition_score_col = None\n",
        "segment_id_col = None\n",
        "road_type_col = None\n",
        "\n",
        "for col in condition_df.columns:\n",
        "    if col.upper() == 'DATE':\n",
        "        date_col = col\n",
        "    elif col.upper() == 'CONDITION_SCORE':\n",
        "        condition_score_col = col\n",
        "    elif col.upper() == 'SEGMENT_ID':\n",
        "        segment_id_col = col\n",
        "\n",
        "for col in roads_df.columns:\n",
        "    if col.upper() == 'ROAD_TYPE':\n",
        "        road_type_col = col\n",
        "    elif col.upper() == 'SEGMENT_ID':\n",
        "        segment_id_col = col\n",
        "\n",
        "print(f\"Detected columns:\")\n",
        "print(f\"  Date: {date_col}\")\n",
        "print(f\"  Condition Score: {condition_score_col}\")\n",
        "print(f\"  Segment ID: {segment_id_col}\")\n",
        "print(f\"  Road Type: {road_type_col}\")\n",
        "\n",
        "# Convert date columns\n",
        "if date_col:\n",
        "    condition_df[date_col] = pd.to_datetime(condition_df[date_col])\n",
        "    print(f\"Date range: {condition_df[date_col].min()} to {condition_df[date_col].max()}\")\n",
        "\n",
        "if 'DATE' in maintenance_df.columns:\n",
        "    maintenance_df['DATE'] = pd.to_datetime(maintenance_df['DATE'])\n",
        "\n",
        "if 'DATE' in traffic_df.columns:\n",
        "    traffic_df['DATE'] = pd.to_datetime(traffic_df['DATE'])\n",
        "\n",
        "print(\"Data preprocessing complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering: Time-based Features\n",
        "print(\"Creating time-based features...\")\n",
        "\n",
        "# Sort by segment and date\n",
        "condition_df = condition_df.sort_values([segment_id_col, date_col])\n",
        "\n",
        "# Time since last inspection\n",
        "condition_df['days_since_last_inspection'] = condition_df.groupby(segment_id_col)[date_col].diff().dt.days\n",
        "\n",
        "# Time-based degradation rate\n",
        "condition_df['condition_change'] = condition_df.groupby(segment_id_col)[condition_score_col].diff()\n",
        "condition_df['degradation_rate'] = condition_df['condition_change'] / condition_df['days_since_last_inspection'].replace(0, np.nan)\n",
        "\n",
        "# Seasonal features\n",
        "condition_df['month'] = condition_df[date_col].dt.month\n",
        "condition_df['quarter'] = condition_df[date_col].dt.quarter\n",
        "condition_df['year'] = condition_df[date_col].dt.year\n",
        "condition_df['day_of_year'] = condition_df[date_col].dt.dayofyear\n",
        "\n",
        "# Season classification\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]:\n",
        "        return 'Winter'\n",
        "    elif month in [3, 4, 5]:\n",
        "        return 'Spring'\n",
        "    elif month in [6, 7, 8]:\n",
        "        return 'Summer'\n",
        "    else:\n",
        "        return 'Fall'\n",
        "\n",
        "condition_df['season'] = condition_df['month'].apply(get_season)\n",
        "\n",
        "print(\"âœ… Time-based features created\")\n",
        "print(f\"  - Days since last inspection: {condition_df['days_since_last_inspection'].notna().sum():,} records\")\n",
        "print(f\"  - Degradation rate: {condition_df['degradation_rate'].notna().sum():,} records\")\n",
        "print(f\"  - Seasonal data: {condition_df['season'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering: Traffic Impact Features\n",
        "print(\"Creating traffic impact features...\")\n",
        "\n",
        "# Merge with traffic data\n",
        "traffic_cols = ['SEGMENT_ID', 'TRAFFIC_VOLUME', 'PEAK_HOUR_FACTOR', 'TRUCK_PERCENTAGE']\n",
        "available_traffic_cols = [col for col in traffic_cols if col in traffic_df.columns]\n",
        "\n",
        "if available_traffic_cols:\n",
        "    # Merge traffic data with condition data\n",
        "    condition_with_traffic = condition_df.merge(\n",
        "        traffic_df[available_traffic_cols], \n",
        "        left_on=segment_id_col, \n",
        "        right_on='SEGMENT_ID', \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Traffic stress features\n",
        "    if 'TRAFFIC_VOLUME' in condition_with_traffic.columns:\n",
        "        condition_with_traffic['traffic_stress'] = condition_with_traffic['TRAFFIC_VOLUME'] * condition_with_traffic.get('PEAK_HOUR_FACTOR', 1.0)\n",
        "        condition_with_traffic['heavy_truck_impact'] = condition_with_traffic['TRAFFIC_VOLUME'] * condition_with_traffic.get('TRUCK_PERCENTAGE', 0.0) / 100\n",
        "        \n",
        "        # Traffic categories\n",
        "        condition_with_traffic['traffic_category'] = pd.cut(\n",
        "            condition_with_traffic['TRAFFIC_VOLUME'], \n",
        "            bins=[0, 1000, 5000, 10000, float('inf')], \n",
        "            labels=['Low', 'Medium', 'High', 'Very High']\n",
        "        )\n",
        "        \n",
        "        print(\"âœ… Traffic features created\")\n",
        "        print(f\"  - Traffic stress: {condition_with_traffic['traffic_stress'].notna().sum():,} records\")\n",
        "        print(f\"  - Heavy truck impact: {condition_with_traffic['heavy_truck_impact'].notna().sum():,} records\")\n",
        "        print(f\"  - Traffic categories: {condition_with_traffic['traffic_category'].value_counts().to_dict()}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Traffic volume data not available\")\n",
        "        condition_with_traffic = condition_df.copy()\n",
        "else:\n",
        "    print(\"âš ï¸ No traffic data available\")\n",
        "    condition_with_traffic = condition_df.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering: Weather Impact Features\n",
        "print(\"Creating weather impact features...\")\n",
        "\n",
        "# Weather damage features (using existing weather columns if available)\n",
        "weather_cols = ['PRECIPITATION', 'FREEZE_THAW_CYCLES', 'TEMPERATURE_AVG']\n",
        "available_weather_cols = [col for col in weather_cols if col in condition_with_traffic.columns]\n",
        "\n",
        "if available_weather_cols:\n",
        "    # Freeze-thaw damage\n",
        "    if 'FREEZE_THAW_CYCLES' in condition_with_traffic.columns:\n",
        "        condition_with_traffic['freeze_thaw_damage'] = condition_with_traffic['FREEZE_THAW_CYCLES'] * 0.1\n",
        "        \n",
        "    # Precipitation impact\n",
        "    if 'PRECIPITATION' in condition_with_traffic.columns:\n",
        "        condition_with_traffic['precipitation_damage'] = condition_with_traffic['PRECIPITATION'] * 0.05\n",
        "        \n",
        "    # Temperature stress\n",
        "    if 'TEMPERATURE_AVG' in condition_with_traffic.columns:\n",
        "        condition_with_traffic['temperature_stress'] = abs(condition_with_traffic['TEMPERATURE_AVG'] - 20) * 0.01\n",
        "        \n",
        "    # Combined weather damage\n",
        "    weather_damage_cols = [col for col in ['freeze_thaw_damage', 'precipitation_damage', 'temperature_stress'] \n",
        "                          if col in condition_with_traffic.columns]\n",
        "    \n",
        "    if weather_damage_cols:\n",
        "        condition_with_traffic['total_weather_damage'] = condition_with_traffic[weather_damage_cols].sum(axis=1)\n",
        "        \n",
        "    print(\"âœ… Weather features created\")\n",
        "    print(f\"  - Available weather columns: {available_weather_cols}\")\n",
        "    print(f\"  - Weather damage features: {len(weather_damage_cols)} created\")\n",
        "else:\n",
        "    print(\"âš ï¸ No weather data available\")\n",
        "    # Create dummy weather features\n",
        "    condition_with_traffic['freeze_thaw_damage'] = 0\n",
        "    condition_with_traffic['precipitation_damage'] = 0\n",
        "    condition_with_traffic['temperature_stress'] = 0\n",
        "    condition_with_traffic['total_weather_damage'] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering: Maintenance History Features\n",
        "print(\"Creating maintenance history features...\")\n",
        "\n",
        "# Merge with maintenance data\n",
        "maintenance_cols = ['SEGMENT_ID', 'DATE', 'COST', 'REPAIR_TYPE']\n",
        "available_maintenance_cols = [col for col in maintenance_cols if col in maintenance_df.columns]\n",
        "\n",
        "if available_maintenance_cols:\n",
        "    # Calculate days since last maintenance\n",
        "    maintenance_df_sorted = maintenance_df.sort_values(['SEGMENT_ID', 'DATE'])\n",
        "    maintenance_df_sorted['days_since_last_maintenance'] = maintenance_df_sorted.groupby('SEGMENT_ID')['DATE'].diff().dt.days\n",
        "    \n",
        "    # Merge maintenance data\n",
        "    condition_with_maintenance = condition_with_traffic.merge(\n",
        "        maintenance_df_sorted[available_maintenance_cols + ['days_since_last_maintenance']], \n",
        "        left_on=[segment_id_col, date_col], \n",
        "        right_on=['SEGMENT_ID', 'DATE'], \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Maintenance frequency features\n",
        "    maintenance_counts = maintenance_df.groupby('SEGMENT_ID').size().reset_index(name='maintenance_frequency')\n",
        "    condition_with_maintenance = condition_with_maintenance.merge(\n",
        "        maintenance_counts, \n",
        "        left_on=segment_id_col, \n",
        "        right_on='SEGMENT_ID', \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Cost features\n",
        "    if 'COST' in condition_with_maintenance.columns:\n",
        "        cost_stats = maintenance_df.groupby('SEGMENT_ID')['COST'].agg(['mean', 'sum', 'count']).reset_index()\n",
        "        cost_stats.columns = ['SEGMENT_ID', 'avg_maintenance_cost', 'total_maintenance_cost', 'maintenance_count']\n",
        "        condition_with_maintenance = condition_with_maintenance.merge(cost_stats, on='SEGMENT_ID', how='left')\n",
        "    \n",
        "    print(\"âœ… Maintenance features created\")\n",
        "    print(f\"  - Days since last maintenance: {condition_with_maintenance['days_since_last_maintenance'].notna().sum():,} records\")\n",
        "    print(f\"  - Maintenance frequency: {condition_with_maintenance['maintenance_frequency'].notna().sum():,} records\")\n",
        "else:\n",
        "    print(\"âš ï¸ No maintenance data available\")\n",
        "    condition_with_maintenance = condition_with_traffic.copy()\n",
        "    # Add dummy maintenance features\n",
        "    condition_with_maintenance['days_since_last_maintenance'] = np.nan\n",
        "    condition_with_maintenance['maintenance_frequency'] = 0\n",
        "    condition_with_maintenance['avg_maintenance_cost'] = 0\n",
        "    condition_with_maintenance['total_maintenance_cost'] = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering: Geospatial and Road Type Features\n",
        "print(\"Creating geospatial and road type features...\")\n",
        "\n",
        "# Debug: Show available columns in road network data\n",
        "print(f\"Available road network columns: {list(roads_df.columns)}\")\n",
        "\n",
        "# Debug: Check segment ID columns and sample data\n",
        "print(f\"Condition data segment ID column: {segment_id_col}\")\n",
        "print(f\"Road network segment ID column: SEGMENT_ID\")\n",
        "print(f\"Sample condition segment IDs: {condition_with_maintenance[segment_id_col].head().tolist()}\")\n",
        "print(f\"Sample road network segment IDs: {roads_df['SEGMENT_ID'].head().tolist()}\")\n",
        "\n",
        "# Check for data type mismatches\n",
        "print(f\"Condition segment ID type: {condition_with_maintenance[segment_id_col].dtype}\")\n",
        "print(f\"Road network segment ID type: {roads_df['SEGMENT_ID'].dtype}\")\n",
        "\n",
        "# FIX: Extract road ID from segment ID for proper matching\n",
        "print(\"ðŸ”§ Fixing segment ID mismatch...\")\n",
        "condition_with_maintenance['ROAD_ID'] = condition_with_maintenance[segment_id_col].str.split('_').str[0]\n",
        "print(f\"Extracted road IDs: {condition_with_maintenance['ROAD_ID'].head().tolist()}\")\n",
        "print(f\"Road network segment IDs: {roads_df['SEGMENT_ID'].head().tolist()}\")\n",
        "\n",
        "# CRITICAL FIX: Deduplicate road network data to prevent Cartesian product\n",
        "print(\"ðŸ”§ Deduplicating road network data...\")\n",
        "print(f\"Road network data before dedup: {len(roads_df):,} records\")\n",
        "roads_df_unique = roads_df.drop_duplicates(subset=['SEGMENT_ID'], keep='first')\n",
        "print(f\"Road network data after dedup: {len(roads_df_unique):,} records\")\n",
        "\n",
        "# Merge with road network data\n",
        "road_cols = ['SEGMENT_ID', 'ROAD_TYPE', 'SEGMENT_LENGTH_MILES', 'LATITUDE', 'LONGITUDE']\n",
        "available_road_cols = [col for col in road_cols if col in roads_df_unique.columns]\n",
        "print(f\"Using road columns: {available_road_cols}\")\n",
        "\n",
        "# Debug: Check what's actually in the deduplicated road network data\n",
        "print(f\"Deduplicated road network columns: {list(roads_df_unique.columns)}\")\n",
        "print(f\"Sample road network data:\")\n",
        "print(roads_df_unique[['SEGMENT_ID', 'ROAD_TYPE', 'SEGMENT_LENGTH_MILES']].head())\n",
        "\n",
        "if available_road_cols:\n",
        "    print(\"Attempting merge with deduplicated road network data...\")\n",
        "    print(f\"Columns being merged: {available_road_cols}\")\n",
        "    print(f\"Sample data being merged:\")\n",
        "    print(roads_df_unique[available_road_cols].head())\n",
        "    \n",
        "    # FIX: Rename road network columns to avoid conflicts\n",
        "    print(\"ðŸ”§ Renaming road network columns to avoid conflicts...\")\n",
        "    road_rename_map = {\n",
        "        'SEGMENT_ID': 'ROAD_SEGMENT_ID',\n",
        "        'ROAD_TYPE': 'ROAD_TYPE',\n",
        "        'SEGMENT_LENGTH_MILES': 'ROAD_LENGTH_MILES',\n",
        "        'LATITUDE': 'ROAD_LATITUDE',\n",
        "        'LONGITUDE': 'ROAD_LONGITUDE'\n",
        "    }\n",
        "    \n",
        "    roads_df_renamed = roads_df_unique[available_road_cols].rename(columns=road_rename_map)\n",
        "    print(f\"Renamed columns: {list(roads_df_renamed.columns)}\")\n",
        "    \n",
        "    condition_final = condition_with_maintenance.merge(\n",
        "        roads_df_renamed, \n",
        "        left_on='ROAD_ID', \n",
        "        right_on='ROAD_SEGMENT_ID', \n",
        "        how='left'\n",
        "    )\n",
        "    \n",
        "    # Debug: Check merge results\n",
        "    print(f\"Merge completed. Final dataset shape: {condition_final.shape}\")\n",
        "    print(f\"Road network columns in merged data: {[col for col in available_road_cols if col in condition_final.columns]}\")\n",
        "    \n",
        "    # Check for successful merges\n",
        "    if 'ROAD_SEGMENT_ID' in condition_final.columns:\n",
        "        successful_merges = condition_final['ROAD_SEGMENT_ID'].notna().sum()\n",
        "        print(f\"Successful road network merges: {successful_merges:,} out of {len(condition_final):,} records\")\n",
        "    else:\n",
        "        print(\"âš ï¸ ROAD_SEGMENT_ID column not found in merged data\")\n",
        "    \n",
        "    # Verify ROAD_TYPE column is present\n",
        "    if 'ROAD_TYPE' in condition_final.columns:\n",
        "        print(f\"âœ… ROAD_TYPE column found! Sample values: {condition_final['ROAD_TYPE'].value_counts().head().to_dict()}\")\n",
        "    else:\n",
        "        print(\"âŒ ROAD_TYPE column still missing after merge\")\n",
        "    \n",
        "    # Verify ROAD_LENGTH_MILES has data\n",
        "    if 'ROAD_LENGTH_MILES' in condition_final.columns:\n",
        "        non_null_lengths = condition_final['ROAD_LENGTH_MILES'].notna().sum()\n",
        "        print(f\"âœ… ROAD_LENGTH_MILES found! Non-null values: {non_null_lengths:,} out of {len(condition_final):,}\")\n",
        "        if non_null_lengths > 0:\n",
        "            print(f\"Length stats: min={condition_final['ROAD_LENGTH_MILES'].min():.2f}, max={condition_final['ROAD_LENGTH_MILES'].max():.2f}\")\n",
        "    else:\n",
        "        print(\"âŒ ROAD_LENGTH_MILES column missing after merge\")\n",
        "    \n",
        "    # Road type features\n",
        "    if 'ROAD_TYPE' in condition_final.columns:\n",
        "        print(\"Creating road type features...\")\n",
        "        print(f\"Road type values: {condition_final['ROAD_TYPE'].value_counts().head()}\")\n",
        "        \n",
        "        condition_final['is_highway'] = condition_final['ROAD_TYPE'].str.contains('Highway', case=False, na=False)\n",
        "        condition_final['is_arterial'] = condition_final['ROAD_TYPE'].str.contains('Arterial', case=False, na=False)\n",
        "        condition_final['is_local'] = condition_final['ROAD_TYPE'].str.contains('Local', case=False, na=False)\n",
        "        \n",
        "        print(f\"Highway segments: {condition_final['is_highway'].sum()}\")\n",
        "        print(f\"Arterial segments: {condition_final['is_arterial'].sum()}\")\n",
        "        print(f\"Local segments: {condition_final['is_local'].sum()}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ ROAD_TYPE column not found in merged data\")\n",
        "    \n",
        "    # Length-based features\n",
        "    if 'ROAD_LENGTH_MILES' in condition_final.columns:\n",
        "        print(\"Creating length categories...\")\n",
        "        print(f\"Length statistics: min={condition_final['ROAD_LENGTH_MILES'].min():.2f}, max={condition_final['ROAD_LENGTH_MILES'].max():.2f}, mean={condition_final['ROAD_LENGTH_MILES'].mean():.2f}\")\n",
        "        \n",
        "        condition_final['length_category'] = pd.cut(\n",
        "            condition_final['ROAD_LENGTH_MILES'], \n",
        "            bins=[0, 0.5, 1.0, 2.0, float('inf')], \n",
        "            labels=['Short', 'Medium', 'Long', 'Very Long']\n",
        "        )\n",
        "        \n",
        "        print(f\"Length categories: {condition_final['length_category'].value_counts().to_dict()}\")\n",
        "    else:\n",
        "        print(\"âš ï¸ ROAD_LENGTH_MILES column not found in merged data\")\n",
        "    \n",
        "    # Geographic features (if coordinates available)\n",
        "    if 'LATITUDE' in condition_final.columns and 'LONGITUDE' in condition_final.columns:\n",
        "        # Simple geographic zones (you could make this more sophisticated)\n",
        "        condition_final['latitude_zone'] = pd.cut(condition_final['LATITUDE'], bins=5, labels=['Zone1', 'Zone2', 'Zone3', 'Zone4', 'Zone5'])\n",
        "        condition_final['longitude_zone'] = pd.cut(condition_final['LONGITUDE'], bins=5, labels=['ZoneA', 'ZoneB', 'ZoneC', 'ZoneD', 'ZoneE'])\n",
        "    \n",
        "    print(\"âœ… Geospatial features created\")\n",
        "    \n",
        "    # Check if road type features were created\n",
        "    road_type_features = ['is_highway', 'is_arterial', 'is_local']\n",
        "    created_road_features = [col for col in road_type_features if col in condition_final.columns]\n",
        "    \n",
        "    if created_road_features:\n",
        "        road_type_count = sum(condition_final[col].sum() for col in created_road_features)\n",
        "        print(f\"  - Road type features: {road_type_count:,} records\")\n",
        "    else:\n",
        "        print(\"  - Road type features: Not created (ROAD_TYPE column not found)\")\n",
        "    \n",
        "    # Check if length categories were created\n",
        "    if 'length_category' in condition_final.columns:\n",
        "        print(f\"  - Length categories: {condition_final['length_category'].value_counts().to_dict()}\")\n",
        "    else:\n",
        "        print(\"  - Length categories: Not created (SEGMENT_LENGTH_MILES column not found)\")\n",
        "else:\n",
        "    print(\"âš ï¸ No road network data available\")\n",
        "    condition_final = condition_with_maintenance.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering: Advanced ML Features (OPTIMIZED for Large Datasets)\n",
        "print(\"ðŸš€ Creating OPTIMIZED ML features for large dataset...\")\n",
        "print(f\"Processing {len(condition_final):,} records across {condition_final[segment_id_col].nunique():,} segments\")\n",
        "\n",
        "# OPTIMIZED: Reduced rolling window features\n",
        "print(\"Creating rolling window features (optimized)...\")\n",
        "window_sizes = [30, 90]  # Reduced from [30, 90, 180] for performance\n",
        "for window in window_sizes:\n",
        "    print(f\"  Processing {window}-day windows...\")\n",
        "    condition_final[f'condition_avg_{window}d'] = condition_final.groupby(segment_id_col)[condition_score_col].rolling(window=window, min_periods=1).mean().reset_index(0, drop=True)\n",
        "    condition_final[f'condition_std_{window}d'] = condition_final.groupby(segment_id_col)[condition_score_col].rolling(window=window, min_periods=1).std().reset_index(0, drop=True)\n",
        "\n",
        "# OPTIMIZED: Reduced lag features\n",
        "print(\"Creating lag features (optimized)...\")\n",
        "for lag in [1, 3, 6]:  # Reduced from [1, 2, 3, 6, 12] for performance\n",
        "    print(f\"  Processing {lag}-month lags...\")\n",
        "    condition_final[f'condition_lag_{lag}m'] = condition_final.groupby(segment_id_col)[condition_score_col].shift(lag)\n",
        "\n",
        "# OPTIMIZED: Simplified trend features\n",
        "print(\"Creating trend features (simplified)...\")\n",
        "condition_final['condition_trend_3m'] = condition_final.groupby(segment_id_col)[condition_score_col].rolling(window=3, min_periods=1).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0).reset_index(0, drop=True)\n",
        "\n",
        "# OPTIMIZED: Simplified interaction features\n",
        "print(\"Creating interaction features...\")\n",
        "if 'traffic_stress' in condition_final.columns and 'total_weather_damage' in condition_final.columns:\n",
        "    condition_final['traffic_weather_interaction'] = condition_final['traffic_stress'] * condition_final['total_weather_damage']\n",
        "\n",
        "# OPTIMIZED: Simplified risk score\n",
        "print(\"Creating risk score...\")\n",
        "risk_components = []\n",
        "if 'traffic_stress' in condition_final.columns:\n",
        "    risk_components.append('traffic_stress')\n",
        "if 'total_weather_damage' in condition_final.columns:\n",
        "    risk_components.append('total_weather_damage')\n",
        "if 'days_since_last_maintenance' in condition_final.columns:\n",
        "    risk_components.append('days_since_last_maintenance')\n",
        "\n",
        "if risk_components:\n",
        "    # Simplified normalization (avoid division by zero)\n",
        "    for component in risk_components:\n",
        "        if component in condition_final.columns:\n",
        "            max_val = condition_final[component].max()\n",
        "            min_val = condition_final[component].min()\n",
        "            if max_val > min_val:\n",
        "                condition_final[f'{component}_normalized'] = (condition_final[component] - min_val) / (max_val - min_val)\n",
        "            else:\n",
        "                condition_final[f'{component}_normalized'] = 0\n",
        "    \n",
        "    normalized_components = [f'{comp}_normalized' for comp in risk_components if f'{comp}_normalized' in condition_final.columns]\n",
        "    if normalized_components:\n",
        "        condition_final['risk_score'] = condition_final[normalized_components].mean(axis=1)\n",
        "\n",
        "print(\"âœ… OPTIMIZED ML features created\")\n",
        "print(f\"  - Rolling window features: {len([col for col in condition_final.columns if 'condition_avg_' in col or 'condition_std_' in col])} features\")\n",
        "print(f\"  - Lag features: {len([col for col in condition_final.columns if 'condition_lag_' in col])} features\")\n",
        "print(f\"  - Risk score: {condition_final['risk_score'].notna().sum():,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed features to Snowflake\n",
        "print(\"Saving processed features to Snowflake...\")\n",
        "\n",
        "# Create a new table for the processed features\n",
        "try:\n",
        "    # Drop the table if it exists\n",
        "    session.sql(\"DROP TABLE IF EXISTS DOT_workshop_test.smartpave_analytics.pavement_features\").collect()\n",
        "    \n",
        "    # Create the table with the processed features\n",
        "    session.create_dataframe(condition_final).write.mode(\"overwrite\").save_as_table(\"DOT_workshop_test.smartpave_analytics.pavement_features\")\n",
        "    \n",
        "    print(\"âœ… Features saved to Snowflake table: pavement_features\")\n",
        "    print(f\"  - Total records: {len(condition_final):,}\")\n",
        "    print(f\"  - Total features: {len(condition_final.columns)}\")\n",
        "    \n",
        "    # Show feature summary\n",
        "    feature_categories = {\n",
        "        'Time-based': [col for col in condition_final.columns if any(x in col for x in ['month', 'quarter', 'year', 'season', 'days_since', 'degradation_rate'])],\n",
        "        'Traffic': [col for col in condition_final.columns if any(x in col for x in ['traffic', 'TRAFFIC_VOLUME', 'PEAK_HOUR', 'TRUCK'])],\n",
        "        'Weather': [col for col in condition_final.columns if any(x in col for x in ['weather', 'freeze', 'precipitation', 'temperature'])],\n",
        "        'Maintenance': [col for col in condition_final.columns if any(x in col for x in ['maintenance', 'COST', 'REPAIR_TYPE'])],\n",
        "        'Geospatial': [col for col in condition_final.columns if any(x in col for x in ['ROAD_TYPE', 'LATITUDE', 'LONGITUDE', 'SEGMENT_LENGTH', 'is_', 'zone'])],\n",
        "        'ML Features': [col for col in condition_final.columns if any(x in col for x in ['condition_avg_', 'condition_std_', 'condition_lag_', 'condition_trend_', 'risk_score', 'interaction'])]\n",
        "    }\n",
        "    \n",
        "    print(\"\\nðŸ“Š Feature Engineering Summary:\")\n",
        "    for category, features in feature_categories.items():\n",
        "        if features:\n",
        "            print(f\"  {category}: {len(features)} features\")\n",
        "            print(f\"    - {', '.join(features[:5])}{'...' if len(features) > 5 else ''}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error saving features: {e}\")\n",
        "    print(\"Features created but not saved to Snowflake\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance estimation for cell 10\n",
        "print(\"ðŸ“Š Performance Estimation for Advanced ML Features\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get record counts\n",
        "total_records = len(condition_final)\n",
        "unique_segments = condition_final[segment_id_col].nunique() if segment_id_col else 0\n",
        "\n",
        "print(f\"Total records: {total_records:,}\")\n",
        "print(f\"Unique segments: {unique_segments:,}\")\n",
        "print(f\"Records per segment (avg): {total_records/unique_segments:.1f}\" if unique_segments > 0 else \"N/A\")\n",
        "\n",
        "# Time estimation based on record count\n",
        "if total_records < 10000:\n",
        "    estimated_time = \"1-3 minutes\"\n",
        "    complexity = \"Low\"\n",
        "elif total_records < 50000:\n",
        "    estimated_time = \"3-8 minutes\"\n",
        "    complexity = \"Medium\"\n",
        "elif total_records < 100000:\n",
        "    estimated_time = \"8-15 minutes\"\n",
        "    complexity = \"High\"\n",
        "else:\n",
        "    estimated_time = \"15+ minutes\"\n",
        "    complexity = \"Very High\"\n",
        "\n",
        "print(f\"\\nâ±ï¸  Estimated processing time: {estimated_time}\")\n",
        "print(f\"ðŸ“ˆ Complexity level: {complexity}\")\n",
        "\n",
        "# Factors affecting performance\n",
        "print(f\"\\nðŸ” Performance factors:\")\n",
        "print(f\"  - Rolling windows: 3 windows Ã— {unique_segments:,} segments\")\n",
        "print(f\"  - Lag features: 5 lags Ã— {total_records:,} records\")\n",
        "print(f\"  - Trend calculations: {unique_segments:,} segments\")\n",
        "print(f\"  - Groupby operations: {unique_segments:,} groups\")\n",
        "\n",
        "if total_records > 50000:\n",
        "    print(f\"\\nðŸ’¡ Consider optimizing if taking too long:\")\n",
        "    print(f\"  - Reduce window sizes (30, 60 days)\")\n",
        "    print(f\"  - Fewer lag features (1, 3, 6 months)\")\n",
        "    print(f\"  - Simpler trend calculation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Performance estimation (can be skipped)\n",
        "print(\"ðŸ“Š Performance Summary\")\n",
        "print(\"=\" * 30)\n",
        "\n",
        "# Get record counts\n",
        "total_records = len(condition_final)\n",
        "unique_segments = condition_final[segment_id_col].nunique() if segment_id_col else 0\n",
        "\n",
        "print(f\"Total records processed: {total_records:,}\")\n",
        "print(f\"Unique segments: {unique_segments:,}\")\n",
        "print(f\"Records per segment (avg): {total_records/unique_segments:.1f}\" if unique_segments > 0 else \"N/A\")\n",
        "\n",
        "# Show what features were created\n",
        "ml_features = [col for col in condition_final.columns if any(x in col for x in ['condition_avg_', 'condition_std_', 'condition_lag_', 'condition_trend_', 'risk_score', 'interaction'])]\n",
        "print(f\"ML features created: {len(ml_features)}\")\n",
        "print(f\"  - Rolling windows: {len([col for col in condition_final.columns if 'condition_avg_' in col or 'condition_std_' in col])}\")\n",
        "print(f\"  - Lag features: {len([col for col in condition_final.columns if 'condition_lag_' in col])}\")\n",
        "print(f\"  - Risk score: {condition_final['risk_score'].notna().sum():,} records\")\n",
        "\n",
        "print(\"\\nâœ… Feature engineering complete!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
